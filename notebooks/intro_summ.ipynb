{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"docs/images/DSPy8.png\" alt=\"DSPy7 Image\" height=\"150\"/>\n",
    "\n",
    "## **DSPy**: Programming with Foundation Models\n",
    "\n",
    "[<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/intro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the **DSPy** framework for **Programming with Foundation Models**, i.e., language models (LMs) and retrieval models (RMs).\n",
    "\n",
    "**DSPy** emphasizes programming over prompting. It unifies techniques for **prompting** and **fine-tuning** LMs as well as improving them with **reasoning** and **tool/retrieval augmentation**, all expressed through a _minimalistic set of Pythonic operations that compose and learn_.\n",
    "\n",
    "**DSPy** provides **composable and declarative modules** for instructing LMs in a familiar Pythonic syntax. On top of that, **DSPy** introduces an **automatic compiler that teaches LMs** how to conduct the declarative steps in your program. The **DSPy compiler** will internally _trace_ your program and then **craft high-quality prompts for large LMs (or train automatic finetunes for small LMs)** to teach them the steps of your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0] Setting Up\n",
    "\n",
    "As we'll start to see below, **DSPy** can routinely teach powerful models like `GPT-3.5` and local models like `T5-base` or `Llama2-13b` to be much more reliable at complex tasks. **DSPy** will compile the _same program_ into different few-shot prompts and/or finetunes for each LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, MIPROv2\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma2:27b'\n",
    "#model_name = 'qwen2.5:72b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not work right now\n",
    "#ollama_port = 11434 \n",
    "#ollama_url = f\"http://localhost:{ollama_port}\"\n",
    "#lm = dspy.LM(model=model_name, api_base=ollama_url)\n",
    "#dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.OllamaLocal(model=model_name)\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build your own **DSPy programs** for various tasks, e.g., question answering, information extraction, or text-to-SQL.\n",
    "\n",
    "Whatever the task, the general workflow is:\n",
    "\n",
    "1. **Collect a little bit of data.** Define examples of the inputs and outputs of your program (e.g., questions and their answers). This could just be a handful of quick examples you wrote down. If large datasets exist, the more the merrier!\n",
    "1. **Write your program.** Define the modules (i.e., sub-tasks) of your program and the way they should interact together to solve your task.\n",
    "1. **Define some validation logic.** What makes for a good run of your program? Maybe the answers need to have a certain length or stick to a particular format? Specify the logic that checks that.\n",
    "1. **Compile!** Ask **DSPy** to _compile_ your program using your data. The compiler will use your data and validation logic to optimize your program (e.g., prompts and modules) so it's efficient and effective!\n",
    "1. **Iterate.** Repeat the process by improving your data, program, validation, or by using more advanced features of the **DSPy** compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'sources', 'sources_metadata', 'summary/long', 'summary/short', 'summary/tiny', 'case_metadata'],\n",
      "        num_rows: 3177\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'sources', 'sources_metadata', 'summary/long', 'summary/short', 'summary/tiny', 'case_metadata'],\n",
      "        num_rows: 454\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'sources', 'sources_metadata', 'summary/long', 'summary/short', 'summary/tiny', 'case_metadata'],\n",
      "        num_rows: 908\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "multi_lexsum = load_dataset(\"allenai/multi_lexsum\", name=\"v20230518\")\n",
    "print(multi_lexsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data type for data in DSPy is Example. You will use Examples to represent items in your training set and test set. DSPy Examples are similar to Python dicts but have a few useful utilities. Your DSPy modules will return values of the type Prediction, which is a special sub-class of Example. When you use DSPy, you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type Example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select 100 train examples and 20 dev examples where all 3 summaries are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = multi_lexsum['train'].filter(\n",
    "    lambda x: x['summary/tiny'] is not None and x['summary/short'] is not None and x['summary/long'] is not None\n",
    ").select(range(100))\n",
    "\n",
    "devset = multi_lexsum['validation'].filter(\n",
    "    lambda x: x['summary/tiny'] is not None and x['summary/short'] is not None and x['summary/long'] is not None\n",
    ").select(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_sources(x):\n",
    "    x['sources'] = ' '.join(x['sources'])\n",
    "    return x\n",
    "\n",
    "trainset = trainset.map(join_sources, batched=False)\n",
    "devset = devset.map(join_sources, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tell DSPy that the joined 'sources' field is the input. Any other fields are labels and/or metadata\n",
    "trainset = [dspy.Example(\n",
    "    doc=x['sources'],\n",
    "    long=x['summary/long'],\n",
    "    short=x['summary/short'],\n",
    "    tiny=x['summary/tiny']\n",
    ").with_inputs('doc') for x in trainset]\n",
    "\n",
    "devset = [dspy.Example(\n",
    "    doc=' '.join(x['sources']),\n",
    "    long=x['summary/long'],\n",
    "    short=x['summary/short'],\n",
    "    tiny=x['summary/tiny']\n",
    ").with_inputs('doc') for x in devset]\n",
    "\n",
    "# ['doc', 'long', 'short', 'tiny']\n",
    "len(trainset), len(devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DSPy** typically requires very minimal labeling. Whereas your pipeline may involve six or seven complex steps, you only need labels for the initial question and the final answer. **DSPy** will bootstrap any intermediate labels needed to support your pipeline. If you change your pipeline in any way, the data bootstrapped will change accordingly!\n",
    "\n",
    "Now, let's look at some data examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short: Two men who were arrested for trespassing on property of businesses open to the public filed a lawsuit in the U.S. District Court for the Western District of Michigan against the city of Grand Rapids, its chief of police, and two individual officers. The plaintiffs claimed that the Grand Rapids Police Department's policy and practice of arresting individuals for trespass -- without probable cause and based on general Letters of Intent to Prosecute signed by Grand Rapids businesses -- results in unreasonable searches and seizures in violation of the Fourth Amendment. The parties came to a private settlement agreement for damages and attorney's fees in late 2019. The Judge dismissed the case in early 2020.\n",
      "tiny: Settlement reached in 2019 for @ACLU case on arrests for trespassing without a warning under Grand Rapids, MI's \"No Trespass Letters\" policy (W.D. Mich.)\n"
     ]
    }
   ],
   "source": [
    "train_example = trainset[1]\n",
    "print(f\"short: {train_example.short}\")\n",
    "print(f\"tiny: {train_example.tiny}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short: Pretrial detainees file lawsuit against Middlesex County in November 2015 to ameliorate the unconstitutional conditions of solitary confinement in the Middlesex County Jail. In September 2018, the parties reached a settlement agreement that restricted the maximum amount of time allowed in isolation and provides those in isolation with opportunities to interact with others.\n",
      "tiny: Pretrial detainees settled this class action against Middlesex County to provide 28 hours per week of out-of-cell time and mental health screenings to people held in solitary confinement.\n"
     ]
    }
   ],
   "source": [
    "dev_example = devset[2]\n",
    "print(f\"short: {dev_example.short}\")\n",
    "print(f\"tiny: {dev_example.tiny}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the raw data, we'd applied `with_inputs(' '.join(x['sources']))` to each example to tell **DSPy** that our input field in each example will be just `doc`. Any other fields are labels or metadata that are not given to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic zero shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiny ~ 25 words\n",
    "\n",
    "short ~ 130 words\n",
    "\n",
    "long ~ 650 words\n",
    "\n",
    "Generate a {summary_type} summary of maximum {max_tokens[summary_type]} tokens of the following text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortSummSig(dspy.Signature):\n",
    "    \"\"\"Generate short summaries of about 130 words.\"\"\"\n",
    "    # input\n",
    "    doc = dspy.InputField()\n",
    "    # output\n",
    "    short = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `ShortSumm`, the docstring describes the sub-task. Each `InputField` or `OutputField` can optionally contain a description `desc` too. When it's not given, it's inferred from the field's name (e.g., `question`).\n",
    "\n",
    "Notice that there isn't anything special about this signature in **DSPy**. We can just as easily define a signature that takes a long snippet from a PDF and outputs structured information, for instance.\n",
    "\n",
    "Anyway, now that we have a signature, let's define and use a **Predictor**. A predictor is a module that knows how to use the LM to implement a signature. Importantly, predictors can **learn** to fit their behavior to the task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dspy.Example(field1=value, field2=value2, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated, underperform, and are about to be deleted. ***\n",
      " \t\tYou are using the client OllamaLocal, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated short: This appears to be a legal document excerpt detailing court activity. Here's a breakdown:\n",
      "\n",
      "* **Dismissal:** The case (likely a civil lawsuit, given the \"cv\" designation) was dismissed with prejudice against Middlesex County. This means the plaintiff cannot refile the same claim.\n",
      "* **Stipulation:**  The dismissal was agreed upon by both parties (the plaintiff and Middlesex County).\n",
      "* **Dates:** Key dates include:\n",
      "    * 10/24/2018: Judge Peter G. Sheridan signed the dismissal order.\n",
      "    * 10/25/2018: The dismissal was officially entered into the court record.\n",
      "* **PACER Transaction:** This section indicates\n",
      "\n",
      "Ground truth short: Pretrial detainees file lawsuit against Middlesex County in November 2015 to ameliorate the unconstitutional conditions of solitary confinement in the Middlesex County Jail. In September 2018, the parties reached a settlement agreement that restricted the maximum amount of time allowed in isolation and provides those in isolation with opportunities to interact with others.\n"
     ]
    }
   ],
   "source": [
    "# Define the predictor.\n",
    "generate_short = dspy.Predict(ShortSummSig)\n",
    "\n",
    "# Call the predictor on a particular input\n",
    "pred = generate_short(doc=dev_example.doc)\n",
    "\n",
    "# Print the prediction\n",
    "#print(f\"Doc: {dev_example.doc}\")\n",
    "print(f\"Generated short: {pred.short}\")\n",
    "print('')\n",
    "print(f\"Ground truth short: {dev_example.short}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongSummSig(dspy.Signature):\n",
    "    \"\"\"Generate long summaries.\"\"\"\n",
    "    # input\n",
    "    doc = dspy.InputField()\n",
    "    # output\n",
    "    long = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated long: This text appears to be a legal document receipt from the PACER (Public Access to Court Electronic Records) system. Here's a breakdown of what it likely means:\n",
      "\n",
      "* **Case Information:** The document refers to a case with docket number \"3:15-cv-07920\". This suggests a civil case filed in the United States District Court for the District of New Jersey (the \"3\" indicates the judicial district).\n",
      "* **Stipulation of Dismissal:**  The text mentions a \"STIPULATION of Dismissal with Prejudice as to Defendant, Middlesex County\". This means that the plaintiff(s) and defendant(s), including Middlesex County, agreed to dismiss the case permanently. \"With prejudice\"\n",
      "\n",
      "Ground truth long: On November 5, 2015, the plaintiffs, nine pretrial detainees, filed this class action in the United States District Court of New Jersey. The plaintiffs sued Middlesex County under 42 U.S.C. § 1983 for the deprivation of rights secured by the Eighth and Fourteenth Amendments to the United States Constitution. The plaintiffs, represented by the ACLU and the New Jersey Office of the Public Defender, asked the court to declare that the conditions in solitary confinement were unconstitutional, to enjoin Middlesex County to take all of its inmates out of solitary confinement, and to award costs and reasonable attorneys' fees. The plaintiffs claimed that the conditions in C-Pod, the solitary confinement unit in Middlesex County Jail, were unconstitutional. Specifically, the plaintiffs claimed that they were locked in a small cell alone almost continuously; could not interact with other inmates; could not be visited by family; could not participate in religious, educational or rehabilitative programs; and were never allowed outdoors. \n",
      "\n",
      "The plaintiffs sought class action status. However, on January 4, 2016, Judge Peter G. Sheridan granted the plaintiffs' request to withdraw their pending motion for class certification without prejudice so that the ACLU attorneys could get acclimated to the case before proceeding with a dispositive motion. \n",
      "\n",
      "The parties then began settlement conversations. On May 24, 2017, the Court stayed discovery in order to further facilitate the parties’ settlement discussions. The parties reached a settlement with eight of the nine plaintiffs signing the agreement by June 19, 2018. The ninth plaintiff indicated he was not unhappy with the agreement, but did not want to sign. After failing to show and respond to requests to explaining absence from a scheduled conference, Magistrate Judge Tonianne J. Bongiovanni recommended this plaintiff’s claims be dismissed with prejudice. 2018 WL 4006809. Judge Peter G. Sheridan adopted the recommendation and dismissed this plaintiff’s claims with prejudice on August 22, 2018. 2018 WL 4005749.\n",
      "\n",
      "The eight remaining plaintiffs entered a private settlement agreement with the defendants on September 25, 2018. Under the settlement agreement, the county would continue to operate a precautionary supervision unit, but also provide 28 hours per week out of cells, access to recreation time, in-unit programming, and law library services. Protections used would involve less restrictive security measures. Disciplinary detention could no longer exceed 15 days for a single disciplinary charge and 30 days for multiple disciplinary charges. The county also implemented a mental health screening policy that allowed the director of mental health to stop inappropriate placements in solitary confinement. The settlement agreement permitted the plaintiffs’ counsel access to people detained in the jail, jail records, and the facility itself to ensure full compliance with the settlement. The defendants agreed to pay $11,123 in attorneys’ fees and cost. Under the agreement, any alleged non-compliance would first be brought to the defendant’s counsel, with the opportunity for mediation. The settlement agreement was enforceable for two years. If the mediation failed to resolve the issue, plaintiffs were permitted to move the Court for reinstatement or ask for a one-time one-year extension of the agreement. The two-year enforcement period began on the date the parties executed the agreement.\n",
      "\n",
      "Judge Peter G. Sheridan entered a stipulated of dismissal on October 25, 2018. As of March 21, 2019, the settlement is still in force.\n"
     ]
    }
   ],
   "source": [
    "# Define the predictor.\n",
    "generate_long = dspy.Predict(LongSummSig)\n",
    "\n",
    "# Call the predictor on a particular input\n",
    "pred = generate_long(doc=dev_example.doc)\n",
    "\n",
    "# Print the prediction\n",
    "#print(f\"Doc: {dev_example.doc}\")\n",
    "print(f\"Generated long: {pred.long}\")\n",
    "print('')\n",
    "print(f\"Ground truth long: {dev_example.long}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def eval_summ_short(example, pred, trace=None):\n",
    "    return scorer.score(example.short.lower(), pred.short.lower())['rouge1'][2]\n",
    "\n",
    "def eval_summ_long(example, pred, trace=None):\n",
    "    return scorer.score(example.long.lower(), pred.long.lower())['rouge1'][2]\n",
    "\n",
    "r1_short = eval_summ_short\n",
    "r1_long = eval_summ_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortSummProg(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_summ = dspy.Predict(ShortSummSig)\n",
    "\n",
    "    def forward(self, doc):\n",
    "        return self.generate_summ(doc=doc)\n",
    "\n",
    "class LongSummProg(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_summ = dspy.Predict(LongSummSig)\n",
    "\n",
    "    def forward(self, doc):\n",
    "        return self.generate_summ(doc=doc)\n",
    "        \n",
    "shortsumm = ShortSummProg()\n",
    "longsumm = LongSummProg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.149412227978533 / 20  (20.7): 100%|█| 20/20 [01:07<00:00,  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.75"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(shortsumm, metric=r1_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.0440304603629977 / 20  (15.2): 100%|█| 20/20 [01:24<00:00,  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.22"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(longsumm, metric=r1_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized few-shot with bootstrapped demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 10 candidate sets.\n"
     ]
    }
   ],
   "source": [
    "bootstrap_optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    max_bootstrapped_demos=8, #8\n",
    "    max_labeled_demos=8, #8\n",
    "    num_candidate_programs=10,\n",
    "    num_threads=8, #8\n",
    "    metric=r1_long,\n",
    "    #teacher_settings=dict(lm=gpt4T)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.9462854710706337 / 20  (14.7): 100%|█| 20/20 [00:34<00:00,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 14.73 for seed -3\n",
      "Scores so far: [14.73]\n",
      "Best score so far: 14.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.0062489253735536 / 20  (15.0): 100%|█| 20/20 [01:42<00:00,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 15.03 for seed -2\n",
      "Scores so far: [14.73, 15.03]\n",
      "Best score so far: 15.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▎                                     | 8/100 [01:17<14:56,  9.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.939228360813264 / 20  (14.7): 100%|█| 20/20 [01:09<00:00,  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [14.73, 15.03, 14.7]\n",
      "Best score so far: 15.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                      | 7/100 [02:06<27:59, 18.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 7 examples for up to 1 rounds, amounting to 7 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.9161932791524565 / 20  (14.6): 100%|█| 20/20 [01:30<00:00,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [14.73, 15.03, 14.7, 14.58]\n",
      "Best score so far: 15.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▏                                       | 3/100 [00:27<14:51,  9.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.984430772350161 / 20  (14.9): 100%|█| 20/20 [01:14<00:00,  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [14.73, 15.03, 14.7, 14.58, 14.92]\n",
      "Best score so far: 15.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                        | 1/100 [00:08<13:51,  8.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.0069943859958026 / 20  (15.0): 100%|█| 20/20 [01:18<00:00,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [14.73, 15.03, 14.7, 14.58, 14.92, 15.03]\n",
      "Best score so far: 15.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▋                                       | 4/100 [01:19<31:56, 19.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.9629981400790464 / 20  (14.8): 100%|█| 20/20 [01:48<00:00,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [14.73, 15.03, 14.7, 14.58, 14.92, 15.03, 14.81]\n",
      "Best score so far: 15.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▋                                       | 4/100 [00:29<11:44,  7.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.9599011735617404 / 20  (14.8): 100%|█| 20/20 [01:19<00:00,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [14.73, 15.03, 14.7, 14.58, 14.92, 15.03, 14.81, 14.8]\n",
      "Best score so far: 15.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██                                       | 5/100 [00:33<10:34,  6.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.9637034399822224 / 20  (14.8): 100%|█| 20/20 [01:43<00:00,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [14.73, 15.03, 14.7, 14.58, 14.92, 15.03, 14.81, 14.8, 14.82]\n",
      "Best score so far: 15.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                        | 2/100 [00:19<15:36,  9.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.9963425914776947 / 20  (15.0): 100%|█| 20/20 [01:11<00:00,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [14.73, 15.03, 14.7, 14.58, 14.92, 15.03, 14.81, 14.8, 14.82, 14.98]\n",
      "Best score so far: 15.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                      | 6/100 [01:54<29:47, 19.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 6 examples for up to 1 rounds, amounting to 6 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.955964425958598 / 20  (14.8): 100%|█| 20/20 [01:53<00:00,  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [14.73, 15.03, 14.7, 14.58, 14.92, 15.03, 14.81, 14.8, 14.82, 14.98, 14.78]\n",
      "Best score so far: 15.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▋                                       | 4/100 [00:59<23:57, 14.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.9649016961062844 / 20  (14.8): 100%|█| 20/20 [01:29<00:00,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [14.73, 15.03, 14.7, 14.58, 14.92, 15.03, 14.81, 14.8, 14.82, 14.98, 14.78, 14.82]\n",
      "Best score so far: 15.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▎                                     | 8/100 [01:18<14:58,  9.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.979916205066145 / 20  (14.9): 100%|█| 20/20 [01:44<00:00,  5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [14.73, 15.03, 14.7, 14.58, 14.92, 15.03, 14.81, 14.8, 14.82, 14.98, 14.78, 14.82, 14.9]\n",
      "Best score so far: 15.03\n",
      "13 candidate programs found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "longsumm_fewshot = bootstrap_optimizer.compile(longsumm, trainset=trainset, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short\n",
    "#max_bootstrapped_demos=4\n",
    "#max_labeled_demos=4\n",
    "21.16\n",
    "\n",
    "#max_bootstrapped_demos=8\n",
    "#max_labeled_demos=8\n",
    "23.4\n",
    "\n",
    "# long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIPROv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing program with MIPRO...\n",
      "\n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "Bootstrapping N=10 sets of demonstrations...\n",
      "Bootstrapping set 1/10\n",
      "Bootstrapping set 2/10\n",
      "Bootstrapping set 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▏                                       | 3/100 [00:15<08:20,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▏                                       | 3/100 [00:25<13:34,  8.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▏                                       | 3/100 [00:20<11:15,  6.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                        | 1/100 [00:15<25:21, 15.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Bootstrapping set 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                        | 1/100 [00:06<11:10,  6.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Bootstrapping set 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                        | 1/100 [00:10<16:43, 10.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Bootstrapping set 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                        | 2/100 [00:14<11:27,  7.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Bootstrapping set 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                        | 1/100 [00:11<18:55, 11.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "\n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "\n",
      "Proposing instructions...\n",
      "\n",
      "Proposed Instructions for Predictor 0:\n",
      "\n",
      "0: Generate long summaries.\n",
      "\n",
      "1: ## PROPOSED INSTRUCTION:\n",
      "\n",
      "You are a legal expert AI assistant tasked with summarizing complex legal cases for easy understanding.  Given a text document describing a legal case, provide a comprehensive long summary that includes the following key elements:\n",
      "\n",
      "* **Parties involved:** Clearly identify the plaintiff(s) and defendant(s), including their roles and affiliations (e.g., incarcerated individual, correctional authorities, legal representation).\n",
      "* **Central issue:** Concisely state the main legal point of contention in the case. For example, is it about housing rights for transgender individuals, healthcare access within correctional facilities, or pandemic-related safety concerns?\n",
      "* **Arguments presented:** Summarize the key arguments made by both sides. What are the plaintiff's claims and the\n",
      "\n",
      "2: Here are some proposed instructions, building on your \"generate long summaries\" idea and incorporating elements of creativity:\n",
      "\n",
      "**Focusing on Narrative & Context:**\n",
      "\n",
      "*  \"Craft a detailed narrative summarizing these legal events as if explaining them to someone unfamiliar with legal proceedings.\"\n",
      "* \"Imagine you are a journalist writing a news article about this court case. Summarize the key developments in a way that is informative and engaging for a general audience.\"\n",
      "\n",
      "**Emphasizing Analysis:**\n",
      "\n",
      "*  \"Analyze the implications of the court's actions on both Brandon Resch and Edward Burley. What might be their next steps?\"\n",
      "* \"Based on the information provided, speculate about the nature of the original lawsuit and the potential issues at stake.\"\n",
      "\n",
      " **\n",
      "\n",
      "3: Here are a few proposed instructions, building on the \"long summary\" concept and aiming for more creativity: \n",
      "\n",
      "**Option 1: Focus on Narrative & Impact**\n",
      "\n",
      "> Summarize this court order as if you were explaining it to someone unfamiliar with legal jargon.  Focus on the story behind the request – why are these FD-302 forms important? What does General Flynn's pardon have to do with it? Finally, describe what impact this order will likely have on both the DOJ and the plaintiffs seeking the documents.\n",
      "\n",
      "**Option 2:  \"Explain Like I'm Five\" Approach**\n",
      "\n",
      "> Imagine you are explaining this court order to a five-year-old. Use simple language and analogies they would understand\n",
      "\n",
      "4: Generate detailed summaries of text, providing comprehensive overviews of key points and events. \n",
      "\n",
      "\n",
      "Let me know if you'd like to test it out with the PACER log example! I can provide a more in-depth summary based on this instruction.\n",
      "\n",
      "5: Generate a comprehensive summary of this legal docket report, explaining the context of the case, the recent motions filed,  the judge's rulings, and any upcoming deadlines. Be sure to identify all parties involved and their respective roles in the case.\n",
      "\n",
      "6: Generate detailed summaries of text, providing context and highlighting key points. \n",
      "\n",
      "\n",
      "Let me know if you'd like me to try summarizing the legal text you provided!\n",
      "\n",
      "7: You are a legal scholar specializing in police reform and community relations. Analyze this excerpt from the Department of Justice report on the Ferguson Police Department, providing a comprehensive summary of its key findings and implications.\n",
      "\n",
      "8: You are a legal scholar specializing in class-action lawsuits.  Analyze this excerpt from a court order and provide a detailed summary of its contents, focusing on the reasons for decertification and the impact of this decision on the parties involved.\n",
      "\n",
      "9: Generate a comprehensive summary of the provided legal text, including details about the case type, parties involved (if identifiable), key motions filed, the court's rulings on those motions, and the final outcome of the case.  Be sure to explain the significance of each entry in chronological order, highlighting any notable agreements or decisions made by the judge.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating the default program...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.0193271000668607 / 20  (15.1): 100%|█| 20/20 [00:33<00:00,  \n",
      "/home/drdo/anaconda3/envs/nlp/lib/python3.12/site-packages/optuna/_experimental.py:30: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default program score: 15.1\n",
      "\n",
      "==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "== Minibatch Trial 1 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.0443032200959197 / 5  (20.9): 100%|█| 5/5 [00:19<00:00,  3.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 20.89 on minibatch of size 5 with parameters ['Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 2'].\n",
      "Minibatch scores so far: [20.89]\n",
      "Full eval scores so far: [15.1]\n",
      "Best full score so far: 15.1\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 2 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.612327645044234 / 5  (12.2): 100%|█| 5/5 [00:21<00:00,  4.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 12.25 on minibatch of size 5 with parameters ['Predictor 1: Instruction 6', 'Predictor 1: Few-Shot Set 2'].\n",
      "Minibatch scores so far: [20.89, 12.25]\n",
      "Full eval scores so far: [15.1]\n",
      "Best full score so far: 15.1\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 3 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.9294036364596564 / 5  (18.6): 100%|█| 5/5 [00:24<00:00,  4.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 18.59 on minibatch of size 5 with parameters ['Predictor 1: Instruction 8', 'Predictor 1: Few-Shot Set 6'].\n",
      "Minibatch scores so far: [20.89, 12.25, 18.59]\n",
      "Full eval scores so far: [15.1]\n",
      "Best full score so far: 15.1\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 4 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.6504209438700116 / 5  (13.0): 100%|█| 5/5 [00:56<00:00, 11.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 13.01 on minibatch of size 5 with parameters ['Predictor 1: Instruction 4', 'Predictor 1: Few-Shot Set 5'].\n",
      "Minibatch scores so far: [20.89, 12.25, 18.59, 13.01]\n",
      "Full eval scores so far: [15.1]\n",
      "Best full score so far: 15.1\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 5 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.650393962959553 / 5  (13.0): 100%|█| 5/5 [00:21<00:00,  4.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 13.01 on minibatch of size 5 with parameters ['Predictor 1: Instruction 3', 'Predictor 1: Few-Shot Set 8'].\n",
      "Minibatch scores so far: [20.89, 12.25, 18.59, 13.01, 13.01]\n",
      "Full eval scores so far: [15.1]\n",
      "Best full score so far: 15.1\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 6 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.7948498106773054 / 5  (15.9): 100%|█| 5/5 [00:15<00:00,  3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 15.9 on minibatch of size 5 with parameters ['Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 3'].\n",
      "Minibatch scores so far: [20.89, 12.25, 18.59, 13.01, 13.01, 15.9]\n",
      "Full eval scores so far: [15.1]\n",
      "Best full score so far: 15.1\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 7 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.8350328634684057 / 5  (16.7): 100%|█| 5/5 [00:54<00:00, 10.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 16.7 on minibatch of size 5 with parameters ['Predictor 1: Instruction 9', 'Predictor 1: Few-Shot Set 5'].\n",
      "Minibatch scores so far: [20.89, 12.25, 18.59, 13.01, 13.01, 15.9, 16.7]\n",
      "Full eval scores so far: [15.1]\n",
      "Best full score so far: 15.1\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 8 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.7558863745657208 / 5  (15.1): 100%|█| 5/5 [00:17<00:00,  3.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 15.12 on minibatch of size 5 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 4'].\n",
      "Minibatch scores so far: [20.89, 12.25, 18.59, 13.01, 13.01, 15.9, 16.7, 15.12]\n",
      "Full eval scores so far: [15.1]\n",
      "Best full score so far: 15.1\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 9 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.7378720097512407 / 5  (14.8): 100%|█| 5/5 [00:36<00:00,  7.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 14.76 on minibatch of size 5 with parameters ['Predictor 1: Instruction 0', 'Predictor 1: Few-Shot Set 7'].\n",
      "Minibatch scores so far: [20.89, 12.25, 18.59, 13.01, 13.01, 15.9, 16.7, 15.12, 14.76]\n",
      "Full eval scores so far: [15.1]\n",
      "Best full score so far: 15.1\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 10 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.6944680231977017 / 5  (13.9): 100%|█| 5/5 [00:37<00:00,  7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 13.89 on minibatch of size 5 with parameters ['Predictor 1: Instruction 9', 'Predictor 1: Few-Shot Set 7'].\n",
      "Minibatch scores so far: [20.89, 12.25, 18.59, 13.01, 13.01, 15.9, 16.7, 15.12, 14.76, 13.89]\n",
      "Full eval scores so far: [15.1]\n",
      "Best full score so far: 15.1\n",
      "=============================\n",
      "\n",
      "\n",
      "===== Full Eval 1 =====\n",
      "Doing full eval on next top averaging program (Avg Score: 20.89) from minibatch trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.5339986207297494 / 16  (15.8):  80%|▊| 16/20 [00:50<00:08,  "
     ]
    }
   ],
   "source": [
    "# Initialize optimizer\n",
    "teleprompter = MIPROv2(\n",
    "    metric=r1_long,\n",
    "    #auto=\"light\", # Can choose between light, medium, and heavy optimization runs\n",
    ")\n",
    "\n",
    "# Optimize program\n",
    "print(f\"Optimizing program with MIPRO...\")\n",
    "optimized_program = teleprompter.compile(\n",
    "    longsumm.deepcopy(),\n",
    "    trainset=trainset,\n",
    "    valset=devset,\n",
    "    minibatch_size=5,\n",
    "    max_bootstrapped_demos=3,\n",
    "    max_labeled_demos=4,\n",
    "    requires_permission_to_run=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimize program for future use\n",
    "#optimized_program.save(f\"mipro_optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
