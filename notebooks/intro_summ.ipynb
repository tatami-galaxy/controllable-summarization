{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"docs/images/DSPy8.png\" alt=\"DSPy7 Image\" height=\"150\"/>\n",
    "\n",
    "## **DSPy**: Programming with Foundation Models\n",
    "\n",
    "[<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/intro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the **DSPy** framework for **Programming with Foundation Models**, i.e., language models (LMs) and retrieval models (RMs).\n",
    "\n",
    "**DSPy** emphasizes programming over prompting. It unifies techniques for **prompting** and **fine-tuning** LMs as well as improving them with **reasoning** and **tool/retrieval augmentation**, all expressed through a _minimalistic set of Pythonic operations that compose and learn_.\n",
    "\n",
    "**DSPy** provides **composable and declarative modules** for instructing LMs in a familiar Pythonic syntax. On top of that, **DSPy** introduces an **automatic compiler that teaches LMs** how to conduct the declarative steps in your program. The **DSPy compiler** will internally _trace_ your program and then **craft high-quality prompts for large LMs (or train automatic finetunes for small LMs)** to teach them the steps of your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0] Setting Up\n",
    "\n",
    "As we'll start to see below, **DSPy** can routinely teach powerful models like `GPT-3.5` and local models like `T5-base` or `Llama2-13b` to be much more reliable at complex tasks. **DSPy** will compile the _same program_ into different few-shot prompts and/or finetunes for each LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 1  (0.0):   5%|▍        | 1/20 [55:26<17:33:29, 3326.81s/it]\n",
      "Average Metric: 0 / 1  (0.0):   5%|▌          | 1/20 [52:06<16:29:56, 3126.15s/it]\n",
      "Average Metric: 0.0 / 4  (0.0):  20%|██▏        | 4/20 [46:52<3:07:31, 703.24s/it]\n",
      "Average Metric: 0.0 / 4  (0.0):  20%|██▏        | 4/20 [30:21<2:01:24, 455.27s/it]\n",
      "Average Metric: 0.0 / 4  (0.0):  20%|██▏        | 4/20 [26:02<1:44:11, 390.75s/it]\n",
      "Average Metric: 0.0 / 4  (0.0):  20%|██▌          | 4/20 [14:25<57:43, 216.44s/it]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058823529411765"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')['rouge1'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma2:27b'\n",
    "#model_name = 'qwen2.5:72b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turbo = dspy.OpenAI(model='gpt-3.5-turbo')\n",
    "lm = dspy.OllamaLocal(model=model_name)\n",
    "dspy.settings.configure(lm=lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build your own **DSPy programs** for various tasks, e.g., question answering, information extraction, or text-to-SQL.\n",
    "\n",
    "Whatever the task, the general workflow is:\n",
    "\n",
    "1. **Collect a little bit of data.** Define examples of the inputs and outputs of your program (e.g., questions and their answers). This could just be a handful of quick examples you wrote down. If large datasets exist, the more the merrier!\n",
    "1. **Write your program.** Define the modules (i.e., sub-tasks) of your program and the way they should interact together to solve your task.\n",
    "1. **Define some validation logic.** What makes for a good run of your program? Maybe the answers need to have a certain length or stick to a particular format? Specify the logic that checks that.\n",
    "1. **Compile!** Ask **DSPy** to _compile_ your program using your data. The compiler will use your data and validation logic to optimize your program (e.g., prompts and modules) so it's efficient and effective!\n",
    "1. **Iterate.** Repeat the process by improving your data, program, validation, or by using more advanced features of the **DSPy** compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'sources', 'sources_metadata', 'summary/long', 'summary/short', 'summary/tiny', 'case_metadata'],\n",
      "        num_rows: 3177\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'sources', 'sources_metadata', 'summary/long', 'summary/short', 'summary/tiny', 'case_metadata'],\n",
      "        num_rows: 454\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'sources', 'sources_metadata', 'summary/long', 'summary/short', 'summary/tiny', 'case_metadata'],\n",
      "        num_rows: 908\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "multi_lexsum = load_dataset(\"allenai/multi_lexsum\", name=\"v20230518\")\n",
    "print(multi_lexsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data type for data in DSPy is Example. You will use Examples to represent items in your training set and test set. DSPy Examples are similar to Python dicts but have a few useful utilities. Your DSPy modules will return values of the type Prediction, which is a special sub-class of Example. When you use DSPy, you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type Example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select 100 train examples and 20 dev examples where all 3 summaries are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = multi_lexsum['train'].filter(\n",
    "    lambda x: x['summary/tiny'] is not None and x['summary/short'] is not None and x['summary/long'] is not None\n",
    ").select(range(100))\n",
    "\n",
    "devset = multi_lexsum['validation'].filter(\n",
    "    lambda x: x['summary/tiny'] is not None and x['summary/short'] is not None and x['summary/long'] is not None\n",
    ").select(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_sources(x):\n",
    "    x['sources'] = ' '.join(x['sources'])\n",
    "    return x\n",
    "\n",
    "trainset = trainset.map(join_sources, batched=False)\n",
    "devset = devset.map(join_sources, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tell DSPy that the joined 'sources' field is the input. Any other fields are labels and/or metadata\n",
    "trainset = [dspy.Example(\n",
    "    doc=x['sources'],\n",
    "    long=x['summary/long'],\n",
    "    short=x['summary/short'],\n",
    "    tiny=x['summary/tiny']\n",
    ").with_inputs('doc') for x in trainset]\n",
    "\n",
    "devset = [dspy.Example(\n",
    "    doc=' '.join(x['sources']),\n",
    "    long=x['summary/long'],\n",
    "    short=x['summary/short'],\n",
    "    tiny=x['summary/tiny']\n",
    ").with_inputs('doc') for x in devset]\n",
    "\n",
    "# ['doc', 'long', 'short', 'tiny']\n",
    "len(trainset), len(devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DSPy** typically requires very minimal labeling. Whereas your pipeline may involve six or seven complex steps, you only need labels for the initial question and the final answer. **DSPy** will bootstrap any intermediate labels needed to support your pipeline. If you change your pipeline in any way, the data bootstrapped will change accordingly!\n",
    "\n",
    "Now, let's look at some data examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short: Two men who were arrested for trespassing on property of businesses open to the public filed a lawsuit in the U.S. District Court for the Western District of Michigan against the city of Grand Rapids, its chief of police, and two individual officers. The plaintiffs claimed that the Grand Rapids Police Department's policy and practice of arresting individuals for trespass -- without probable cause and based on general Letters of Intent to Prosecute signed by Grand Rapids businesses -- results in unreasonable searches and seizures in violation of the Fourth Amendment. The parties came to a private settlement agreement for damages and attorney's fees in late 2019. The Judge dismissed the case in early 2020.\n",
      "tiny: Settlement reached in 2019 for @ACLU case on arrests for trespassing without a warning under Grand Rapids, MI's \"No Trespass Letters\" policy (W.D. Mich.)\n"
     ]
    }
   ],
   "source": [
    "train_example = trainset[1]\n",
    "print(f\"short: {train_example.short}\")\n",
    "print(f\"tiny: {train_example.tiny}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short: Pretrial detainees file lawsuit against Middlesex County in November 2015 to ameliorate the unconstitutional conditions of solitary confinement in the Middlesex County Jail. In September 2018, the parties reached a settlement agreement that restricted the maximum amount of time allowed in isolation and provides those in isolation with opportunities to interact with others.\n",
      "tiny: Pretrial detainees settled this class action against Middlesex County to provide 28 hours per week of out-of-cell time and mental health screenings to people held in solitary confinement.\n"
     ]
    }
   ],
   "source": [
    "dev_example = devset[2]\n",
    "print(f\"short: {dev_example.short}\")\n",
    "print(f\"tiny: {dev_example.tiny}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the raw data, we'd applied `with_inputs(' '.join(x['sources']))` to each example to tell **DSPy** that our input field in each example will be just `doc`. Any other fields are labels or metadata that are not given to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic zero shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiny ~ 25 words\n",
    "\n",
    "short ~ 130 words\n",
    "\n",
    "long ~ 650 words\n",
    "\n",
    "Generate a {summary_type} summary of maximum {max_tokens[summary_type]} tokens of the following text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortSummSig(dspy.Signature):\n",
    "    \"\"\"Generate short summaries of about 130 words.\"\"\"\n",
    "    # input\n",
    "    doc = dspy.InputField()\n",
    "    # output\n",
    "    short = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `ShortSumm`, the docstring describes the sub-task. Each `InputField` or `OutputField` can optionally contain a description `desc` too. When it's not given, it's inferred from the field's name (e.g., `question`).\n",
    "\n",
    "Notice that there isn't anything special about this signature in **DSPy**. We can just as easily define a signature that takes a long snippet from a PDF and outputs structured information, for instance.\n",
    "\n",
    "Anyway, now that we have a signature, let's define and use a **Predictor**. A predictor is a module that knows how to use the LM to implement a signature. Importantly, predictors can **learn** to fit their behavior to the task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dspy.Example(field1=value, field2=value2, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated short: This appears to be a legal document excerpt detailing court activity. Here's a breakdown:\n",
      "\n",
      "* **Dismissal:** The case (likely a civil lawsuit, given the \"cv\" designation) was dismissed with prejudice against Middlesex County. This means the plaintiff cannot refile the same claim.\n",
      "* **Stipulation:**  The dismissal was agreed upon by both parties (the plaintiff and Middlesex County).\n",
      "* **Dates:** Key dates include:\n",
      "    * 10/24/2018: Judge Peter G. Sheridan signed the dismissal order.\n",
      "    * 10/25/2018: The dismissal was officially entered into the court record.\n",
      "* **PACER Transaction:** This section indicates\n",
      "\n",
      "Ground truth short: Pretrial detainees file lawsuit against Middlesex County in November 2015 to ameliorate the unconstitutional conditions of solitary confinement in the Middlesex County Jail. In September 2018, the parties reached a settlement agreement that restricted the maximum amount of time allowed in isolation and provides those in isolation with opportunities to interact with others.\n"
     ]
    }
   ],
   "source": [
    "# Define the predictor.\n",
    "generate_short = dspy.Predict(ShortSummSig)\n",
    "\n",
    "# Call the predictor on a particular input\n",
    "pred = generate_short(doc=dev_example.doc)\n",
    "\n",
    "# Print the prediction\n",
    "#print(f\"Doc: {dev_example.doc}\")\n",
    "print(f\"Generated short: {pred.short}\")\n",
    "print('')\n",
    "print(f\"Ground truth short: {dev_example.short}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def eval_summ(example, pred, trace=None):\n",
    "    return scorer.score(example.short.lower(), pred.short.lower())['rouge1'][2]\n",
    "\n",
    "r1 = eval_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortSummProg(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_summ = dspy.Predict(ShortSummSig)\n",
    "\n",
    "    def forward(self, doc):\n",
    "        return self.generate_summ(doc=doc)\n",
    "\n",
    "shortsumm = ShortSummProg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.149412227978533 / 20  (20.7): 100%|█| 20/20 [01:05<00:00,  3.27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.75"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(shortsumm, metric=r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized few-shot with bootstrapped demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
