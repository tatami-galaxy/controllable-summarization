{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"docs/images/DSPy8.png\" alt=\"DSPy7 Image\" height=\"150\"/>\n",
    "\n",
    "## **DSPy**: Programming with Foundation Models\n",
    "\n",
    "[<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/intro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the **DSPy** framework for **Programming with Foundation Models**, i.e., language models (LMs) and retrieval models (RMs).\n",
    "\n",
    "**DSPy** emphasizes programming over prompting. It unifies techniques for **prompting** and **fine-tuning** LMs as well as improving them with **reasoning** and **tool/retrieval augmentation**, all expressed through a _minimalistic set of Pythonic operations that compose and learn_.\n",
    "\n",
    "**DSPy** provides **composable and declarative modules** for instructing LMs in a familiar Pythonic syntax. On top of that, **DSPy** introduces an **automatic compiler that teaches LMs** how to conduct the declarative steps in your program. The **DSPy compiler** will internally _trace_ your program and then **craft high-quality prompts for large LMs (or train automatic finetunes for small LMs)** to teach them the steps of your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0] Setting Up\n",
    "\n",
    "As we'll start to see below, **DSPy** can routinely teach powerful models like `GPT-3.5` and local models like `T5-base` or `Llama2-13b` to be much more reliable at complex tasks. **DSPy** will compile the _same program_ into different few-shot prompts and/or finetunes for each LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, MIPROv2\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemma2:27b'\n",
    "#model_name = 'qwen2.5:72b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not work right now\n",
    "#ollama_port = 11434 \n",
    "#ollama_url = f\"http://localhost:{ollama_port}\"\n",
    "#lm = dspy.LM(model=model_name, api_base=ollama_url)\n",
    "#dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.OllamaLocal(model=model_name)\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build your own **DSPy programs** for various tasks, e.g., question answering, information extraction, or text-to-SQL.\n",
    "\n",
    "Whatever the task, the general workflow is:\n",
    "\n",
    "1. **Collect a little bit of data.** Define examples of the inputs and outputs of your program (e.g., questions and their answers). This could just be a handful of quick examples you wrote down. If large datasets exist, the more the merrier!\n",
    "1. **Write your program.** Define the modules (i.e., sub-tasks) of your program and the way they should interact together to solve your task.\n",
    "1. **Define some validation logic.** What makes for a good run of your program? Maybe the answers need to have a certain length or stick to a particular format? Specify the logic that checks that.\n",
    "1. **Compile!** Ask **DSPy** to _compile_ your program using your data. The compiler will use your data and validation logic to optimize your program (e.g., prompts and modules) so it's efficient and effective!\n",
    "1. **Iterate.** Repeat the process by improving your data, program, validation, or by using more advanced features of the **DSPy** compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'sources', 'sources_metadata', 'summary/long', 'summary/short', 'summary/tiny', 'case_metadata'],\n",
      "        num_rows: 3177\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'sources', 'sources_metadata', 'summary/long', 'summary/short', 'summary/tiny', 'case_metadata'],\n",
      "        num_rows: 454\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'sources', 'sources_metadata', 'summary/long', 'summary/short', 'summary/tiny', 'case_metadata'],\n",
      "        num_rows: 908\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "multi_lexsum = load_dataset(\"allenai/multi_lexsum\", name=\"v20230518\")\n",
    "print(multi_lexsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data type for data in DSPy is Example. You will use Examples to represent items in your training set and test set. DSPy Examples are similar to Python dicts but have a few useful utilities. Your DSPy modules will return values of the type Prediction, which is a special sub-class of Example. When you use DSPy, you will do a lot of evaluation and optimization runs. Your individual datapoints will be of type Example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select 100 train examples and 20 dev examples where all 3 summaries are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = multi_lexsum['train'].filter(\n",
    "    lambda x: x['summary/tiny'] is not None and x['summary/short'] is not None and x['summary/long'] is not None\n",
    ").select(range(100))\n",
    "\n",
    "devset = multi_lexsum['validation'].filter(\n",
    "    lambda x: x['summary/tiny'] is not None and x['summary/short'] is not None and x['summary/long'] is not None\n",
    ").select(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_sources(x):\n",
    "    x['sources'] = ' '.join(x['sources'])\n",
    "    return x\n",
    "\n",
    "trainset = trainset.map(join_sources, batched=False)\n",
    "devset = devset.map(join_sources, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tell DSPy that the joined 'sources' field is the input. Any other fields are labels and/or metadata\n",
    "trainset = [dspy.Example(\n",
    "    doc=x['sources'],\n",
    "    long=x['summary/long'],\n",
    "    short=x['summary/short'],\n",
    "    tiny=x['summary/tiny']\n",
    ").with_inputs('doc') for x in trainset]\n",
    "\n",
    "devset = [dspy.Example(\n",
    "    doc=' '.join(x['sources']),\n",
    "    long=x['summary/long'],\n",
    "    short=x['summary/short'],\n",
    "    tiny=x['summary/tiny']\n",
    ").with_inputs('doc') for x in devset]\n",
    "\n",
    "# ['doc', 'long', 'short', 'tiny']\n",
    "len(trainset), len(devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DSPy** typically requires very minimal labeling. Whereas your pipeline may involve six or seven complex steps, you only need labels for the initial question and the final answer. **DSPy** will bootstrap any intermediate labels needed to support your pipeline. If you change your pipeline in any way, the data bootstrapped will change accordingly!\n",
    "\n",
    "Now, let's look at some data examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short: Two men who were arrested for trespassing on property of businesses open to the public filed a lawsuit in the U.S. District Court for the Western District of Michigan against the city of Grand Rapids, its chief of police, and two individual officers. The plaintiffs claimed that the Grand Rapids Police Department's policy and practice of arresting individuals for trespass -- without probable cause and based on general Letters of Intent to Prosecute signed by Grand Rapids businesses -- results in unreasonable searches and seizures in violation of the Fourth Amendment. The parties came to a private settlement agreement for damages and attorney's fees in late 2019. The Judge dismissed the case in early 2020.\n",
      "tiny: Settlement reached in 2019 for @ACLU case on arrests for trespassing without a warning under Grand Rapids, MI's \"No Trespass Letters\" policy (W.D. Mich.)\n"
     ]
    }
   ],
   "source": [
    "train_example = trainset[1]\n",
    "print(f\"short: {train_example.short}\")\n",
    "print(f\"tiny: {train_example.tiny}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short: Pretrial detainees file lawsuit against Middlesex County in November 2015 to ameliorate the unconstitutional conditions of solitary confinement in the Middlesex County Jail. In September 2018, the parties reached a settlement agreement that restricted the maximum amount of time allowed in isolation and provides those in isolation with opportunities to interact with others.\n",
      "tiny: Pretrial detainees settled this class action against Middlesex County to provide 28 hours per week of out-of-cell time and mental health screenings to people held in solitary confinement.\n"
     ]
    }
   ],
   "source": [
    "dev_example = devset[2]\n",
    "print(f\"short: {dev_example.short}\")\n",
    "print(f\"tiny: {dev_example.tiny}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the raw data, we'd applied `with_inputs(' '.join(x['sources']))` to each example to tell **DSPy** that our input field in each example will be just `doc`. Any other fields are labels or metadata that are not given to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic zero shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiny ~ 25 words\n",
    "\n",
    "short ~ 130 words\n",
    "\n",
    "long ~ 650 words\n",
    "\n",
    "Generate a {summary_type} summary of maximum {max_tokens[summary_type]} tokens of the following text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortSummSig(dspy.Signature):\n",
    "    \"\"\"Generate short summaries of about 130 words.\"\"\"\n",
    "    # input\n",
    "    doc = dspy.InputField()\n",
    "    # output\n",
    "    short = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `ShortSumm`, the docstring describes the sub-task. Each `InputField` or `OutputField` can optionally contain a description `desc` too. When it's not given, it's inferred from the field's name (e.g., `question`).\n",
    "\n",
    "Notice that there isn't anything special about this signature in **DSPy**. We can just as easily define a signature that takes a long snippet from a PDF and outputs structured information, for instance.\n",
    "\n",
    "Anyway, now that we have a signature, let's define and use a **Predictor**. A predictor is a module that knows how to use the LM to implement a signature. Importantly, predictors can **learn** to fit their behavior to the task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dspy.Example(field1=value, field2=value2, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemma2:27b\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m generate_short \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPredict(ShortSummSig)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Call the predictor on a particular input\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_short\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_example\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Print the prediction\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#print(f\"Doc: {dev_example.doc}\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated short: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39mshort\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/dspy/utils/callback.py:202\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/dspy/predict/predict.py:121\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/dspy/predict/predict.py:155\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lm, dspy\u001b[38;5;241m.\u001b[39mLM):\n\u001b[0;32m--> 155\u001b[0m     completions \u001b[38;5;241m=\u001b[39m \u001b[43mv2_5_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_parse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     warn_once(\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munderperform, and are about to be deleted. ***\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mhttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/dspy/predict/predict.py:264\u001b[0m, in \u001b[0;36mv2_5_generate\u001b[0;34m(lm, lm_kwargs, signature, demos, inputs, _parse_values)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\n\u001b[1;32m    262\u001b[0m adapter \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39madapter \u001b[38;5;129;01mor\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39mChatAdapter()\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43madapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_parse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_parse_values\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/dspy/adapters/base.py:19\u001b[0m, in \u001b[0;36mAdapter.__call__\u001b[0;34m(self, lm, lm_kwargs, signature, demos, inputs, _parse_values)\u001b[0m\n\u001b[1;32m     16\u001b[0m inputs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(signature, demos, inputs)\n\u001b[1;32m     17\u001b[0m inputs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(prompt\u001b[38;5;241m=\u001b[39minputs_) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs_, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(messages\u001b[38;5;241m=\u001b[39minputs_)\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/dspy/utils/callback.py:202\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/dspy/clients/lm.py:71\u001b[0m, in \u001b[0;36mLM.__call__\u001b[0;34m(self, prompt, messages, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     completion \u001b[38;5;241m=\u001b[39m cached_litellm_text_completion \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;28;01melse\u001b[39;00m litellm_text_completion\n\u001b[0;32m---> 71\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mujson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(c, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Logging, with removed api key & where `cost` is None on cache hit.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/dspy/clients/lm.py:159\u001b[0m, in \u001b[0;36mcached_litellm_completion\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_litellm_completion\u001b[39m(request):\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno-cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno-store\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/dspy/clients/lm.py:164\u001b[0m, in \u001b[0;36mlitellm_completion\u001b[0;34m(request, cache)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlitellm_completion\u001b[39m(request, cache\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno-cache\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno-store\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}):\n\u001b[1;32m    163\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m ujson\u001b[38;5;241m.\u001b[39mloads(request)\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/litellm/utils.py:1070\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1067\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1068\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1069\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1070\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/litellm/utils.py:958\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    956\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    957\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 958\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/litellm/main.py:2957\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2956\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 2957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[1;32m   2958\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   2959\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   2960\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   2961\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2962\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   2963\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/litellm/main.py:852\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m     model \u001b[38;5;241m=\u001b[39m deployment_id\n\u001b[1;32m    851\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 852\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hidden_params\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    859\u001b[0m     model_response\u001b[38;5;241m.\u001b[39m_hidden_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_llm_provider\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m custom_llm_provider\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:520\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[0;32m--> 520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m         error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    523\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    524\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:497\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    495\u001b[0m     error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Pass model as E.g. For \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuggingface\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m inference endpoints pass in `completion(model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/starcoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    498\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    499\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    500\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[1;32m    501\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m    502\u001b[0m             content\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    503\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    504\u001b[0m         ),\n\u001b[1;32m    505\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(api_base)\n\u001b[1;32m    510\u001b[0m     )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=gemma2:27b\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "# Define the predictor.\n",
    "generate_short = dspy.Predict(ShortSummSig)\n",
    "\n",
    "# Call the predictor on a particular input\n",
    "pred = generate_short(doc=dev_example.doc)\n",
    "\n",
    "# Print the prediction\n",
    "#print(f\"Doc: {dev_example.doc}\")\n",
    "print(f\"Generated short: {pred.short}\")\n",
    "print('')\n",
    "print(f\"Ground truth short: {dev_example.short}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "def eval_summ(example, pred, trace=None):\n",
    "    return scorer.score(example.short.lower(), pred.short.lower())['rouge1'][2]\n",
    "\n",
    "r1 = eval_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortSummProg(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_summ = dspy.Predict(ShortSummSig)\n",
    "\n",
    "    def forward(self, doc):\n",
    "        return self.generate_summ(doc=doc)\n",
    "\n",
    "shortsumm = ShortSummProg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.149412227978533 / 20  (20.7): 100%|████| 20/20 [01:06<00:00,  3.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.75"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(shortsumm, metric=r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized few-shot with bootstrapped demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n"
     ]
    }
   ],
   "source": [
    "bootstrap_optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    max_bootstrapped_demos=8, #8\n",
    "    max_labeled_demos=8, #8\n",
    "    #num_candidate_programs=10,\n",
    "    num_threads=8, #8\n",
    "    metric=r1,\n",
    "    #teacher_settings=dict(lm=gpt4T)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortsumm_fewshot = bootstrap_optimizer.compile(shortsumm, trainset=trainset, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_bootstrapped_demos=4\n",
    "#max_labeled_demos=4\n",
    "21.16\n",
    "\n",
    "#max_bootstrapped_demos=8\n",
    "#max_labeled_demos=8\n",
    "23.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIPROv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing program with MIPRO...\n",
      "\n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "Bootstrapping N=10 sets of demonstrations...\n",
      "Bootstrapping set 1/10\n",
      "Bootstrapping set 2/10\n",
      "Bootstrapping set 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▌                                                  | 3/100 [00:15<08:19,  5.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▌                                                  | 3/100 [00:23<12:30,  7.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▌                                                  | 3/100 [00:19<10:41,  6.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                   | 1/100 [00:15<24:59, 15.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Bootstrapping set 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                   | 1/100 [00:06<10:42,  6.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Bootstrapping set 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                   | 1/100 [00:08<14:22,  8.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Bootstrapping set 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█                                                   | 2/100 [00:13<11:02,  6.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Bootstrapping set 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                   | 1/100 [00:11<18:19, 11.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "\n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "\n",
      "Proposing instructions...\n",
      "\n",
      "Proposed Instructions for Predictor 0:\n",
      "\n",
      "0: Generate short summaries of about 130 words.\n",
      "\n",
      "1: ## PROPOSED INSTRUCTION:\n",
      "\n",
      "You are a legal expert assistant tasked with summarizing complex legal documents for a general audience. Your goal is to create concise and engaging summaries of approximately 130 words.  \n",
      "\n",
      "**Here's what I need from you:**\n",
      "\n",
      "* **Clarity and Simplicity:** Use clear, concise language and avoid unnecessary legal jargon.\n",
      "* **Engaging Hook:** Start with a sentence or two that grabs the reader's attention and briefly explains the document's topic. Consider using an \"@\" mention format to personalize the summary (e.g., \"@individuals seeking information about...\" or \"@businesses facing...\").\n",
      "\n",
      "* **Key Points:** Highlight the most important points of the legal document in a logical order.\n",
      "* **Impact\n",
      "\n",
      "2: ## PROPOSED INSTRUCTION: \n",
      "\n",
      "Imagine you are a paralegal summarizing case updates for a busy attorney.  Write a concise summary (around 130 words) of the provided legal document log, focusing on the key actions taken and their potential implications for the case. Be sure to highlight any denied motions or adopted recommendations.\n",
      "\n",
      "3: Here are a few proposed instructions, each with a slightly different spin:\n",
      "\n",
      "**Option 1 (Focus on Clarity):**\n",
      "\n",
      "Summarize this court order in approximately 130 words, ensuring the summary is clear and concise for someone unfamiliar with legal jargon.  \n",
      "\n",
      "**Option 2 (Emphasis on Impact):**\n",
      "\n",
      "Explain the key implications of this court order regarding the release of FBI FD-302 forms from the Mueller investigation. Keep your summary around 130 words.\n",
      "\n",
      "**Option 3 (Narrative Approach):**\n",
      "\n",
      "Imagine you are explaining this court order to a friend who is interested in the Mueller investigation.  Write a brief, 130-word summary that captures the essence of the order and its\n",
      "\n",
      "4: Summarize the legal activity in this case excerpt, focusing on the key events and motions filed. Limit your summary to approximately 130 words.\n",
      "\n",
      "5: Generate a concise summary of the legal document excerpt, focusing on the key events and deadlines.  Include information about the case type, involved parties (e.g., Monitor, USDOJ, Commonwealth), and any relevant court orders or rulings. The summary should be approximately 130 words in length.\n",
      "\n",
      "For example, you could start with: \"This excerpt details activity in a civil case (3:94-cv-02080-GAG) filed in 1994.  Judge Gustavo A. Gelpi approved an invoice submitted by court-appointed monitor Kimberly Tandy...\n",
      "\n",
      "6: Summarize the following legal document in approximately 130 words. Focus on key actions, parties involved, and any potential issues raised.\n",
      "\n",
      "7: You are a journalist writing a concise news summary for your readers.  Generate a short summary of about 130 words based on the provided text.\n",
      "\n",
      "8: You are a legal journalist tasked with writing a concise summary of a court ruling for a general audience.  Generate a short summary (approximately 130 words) of this court document, explaining the key points in clear and understandable language. Avoid using excessive legal jargon.\n",
      "\n",
      "9: ##  Proposed Instruction:\n",
      "\n",
      "Generate a concise summary (approximately 130 words) of the provided legal text, highlighting the following:\n",
      "\n",
      "* **Case Name & Number:** Identify the case name and number.\n",
      "* **Key Dates:** List significant dates mentioned in the document, including hearings, filings, and orders.\n",
      "* **Parties Involved:** Briefly mention the key parties involved (plaintiffs, defendants, attorneys).\n",
      "* **Nature of Proceedings:** Describe the type of legal issues addressed (e.g., motion to strike lien, settlement agreement, attorney fees).\n",
      "* **Outcome:** Summarize the final resolution of the case, including any orders or judgments issued by the judge.\n",
      "\n",
      "**Example:** \n",
      "\n",
      "\n",
      "\"This summary should\n",
      "\n",
      "\n",
      "\n",
      "Evaluating the default program...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.149165274748174 / 20  (20.7): 100%|████| 20/20 [00:30<00:00,  1.50s/it]\n",
      "/home/drdo/anaconda3/envs/nlp/lib/python3.12/site-packages/optuna/_experimental.py:30: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default program score: 20.75\n",
      "\n",
      "==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "== Minibatch Trial 1 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.401343064094742 / 5  (28.0): 100%|███████| 5/5 [00:18<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 28.03 on minibatch of size 5 with parameters ['Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 2'].\n",
      "Minibatch scores so far: [28.03]\n",
      "Full eval scores so far: [20.75]\n",
      "Best full score so far: 20.75\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 2 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.0618074590430882 / 5  (21.2): 100%|██████| 5/5 [00:20<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 21.24 on minibatch of size 5 with parameters ['Predictor 1: Instruction 6', 'Predictor 1: Few-Shot Set 2'].\n",
      "Minibatch scores so far: [28.03, 21.24]\n",
      "Full eval scores so far: [20.75]\n",
      "Best full score so far: 20.75\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 3 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.2204817001949637 / 5  (24.4): 100%|██████| 5/5 [00:24<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 24.41 on minibatch of size 5 with parameters ['Predictor 1: Instruction 8', 'Predictor 1: Few-Shot Set 6'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41]\n",
      "Full eval scores so far: [20.75]\n",
      "Best full score so far: 20.75\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 4 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.9273677864618655 / 5  (18.5): 100%|██████| 5/5 [00:55<00:00, 11.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 18.55 on minibatch of size 5 with parameters ['Predictor 1: Instruction 4', 'Predictor 1: Few-Shot Set 5'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55]\n",
      "Full eval scores so far: [20.75]\n",
      "Best full score so far: 20.75\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 5 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.8120506925572133 / 5  (16.2): 100%|██████| 5/5 [00:18<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 16.24 on minibatch of size 5 with parameters ['Predictor 1: Instruction 3', 'Predictor 1: Few-Shot Set 8'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24]\n",
      "Full eval scores so far: [20.75]\n",
      "Best full score so far: 20.75\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 6 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.1770990112609483 / 5  (23.5): 100%|██████| 5/5 [00:13<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 23.54 on minibatch of size 5 with parameters ['Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 3'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54]\n",
      "Full eval scores so far: [20.75]\n",
      "Best full score so far: 20.75\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 7 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.036908424788845 / 5  (20.7): 100%|███████| 5/5 [00:52<00:00, 10.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 20.74 on minibatch of size 5 with parameters ['Predictor 1: Instruction 9', 'Predictor 1: Few-Shot Set 5'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74]\n",
      "Full eval scores so far: [20.75]\n",
      "Best full score so far: 20.75\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 8 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.4303765880779884 / 5  (28.6): 100%|██████| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 28.61 on minibatch of size 5 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 4'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61]\n",
      "Full eval scores so far: [20.75]\n",
      "Best full score so far: 20.75\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 9 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.2710077590105233 / 5  (25.4): 100%|██████| 5/5 [00:33<00:00,  6.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 25.42 on minibatch of size 5 with parameters ['Predictor 1: Instruction 0', 'Predictor 1: Few-Shot Set 7'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42]\n",
      "Full eval scores so far: [20.75]\n",
      "Best full score so far: 20.75\n",
      "============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 10 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.9475080447568882 / 5  (19.0): 100%|██████| 5/5 [00:34<00:00,  6.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 18.95 on minibatch of size 5 with parameters ['Predictor 1: Instruction 9', 'Predictor 1: Few-Shot Set 7'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95]\n",
      "Full eval scores so far: [20.75]\n",
      "Best full score so far: 20.75\n",
      "=============================\n",
      "\n",
      "\n",
      "===== Full Eval 1 =====\n",
      "Doing full eval on next top averaging program (Avg Score: 28.61) from minibatch trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.3351003110630595 / 20  (21.7): 100%|███| 20/20 [00:48<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mNew best full eval score!\u001b[0m Score: 21.68\n",
      "Full eval scores so far: [20.75, 21.68]\n",
      "Best full score so far: 21.68\n",
      "=======================\n",
      "\n",
      "\n",
      "== Minibatch Trial 11 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.078304408517299 / 5  (21.6): 100%|███████| 5/5 [00:17<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 21.57 on minibatch of size 5 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 9'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57]\n",
      "Full eval scores so far: [20.75, 21.68]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 12 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.9603992666113574 / 5  (19.2): 100%|██████| 5/5 [00:13<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 19.21 on minibatch of size 5 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 4'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21]\n",
      "Full eval scores so far: [20.75, 21.68]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 13 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.3315784517141984 / 5  (26.6): 100%|██████| 5/5 [00:11<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 26.63 on minibatch of size 5 with parameters ['Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 1'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63]\n",
      "Full eval scores so far: [20.75, 21.68]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 14 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.8009034851804644 / 5  (16.0): 100%|██████| 5/5 [00:18<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 16.02 on minibatch of size 5 with parameters ['Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 2'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02]\n",
      "Full eval scores so far: [20.75, 21.68]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 15 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.1713741409924427 / 5  (23.4): 100%|██████| 5/5 [00:12<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 23.43 on minibatch of size 5 with parameters ['Predictor 1: Instruction 5', 'Predictor 1: Few-Shot Set 4'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43]\n",
      "Full eval scores so far: [20.75, 21.68]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 16 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.1144951490546016 / 5  (22.3): 100%|██████| 5/5 [00:14<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 22.29 on minibatch of size 5 with parameters ['Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 4'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29]\n",
      "Full eval scores so far: [20.75, 21.68]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 17 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.1445684750032576 / 5  (22.9): 100%|██████| 5/5 [00:06<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 22.89 on minibatch of size 5 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 0'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89]\n",
      "Full eval scores so far: [20.75, 21.68]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 18 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.0020107023441176 / 5  (20.0): 100%|██████| 5/5 [00:16<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 20.04 on minibatch of size 5 with parameters ['Predictor 1: Instruction 8', 'Predictor 1: Few-Shot Set 2'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04]\n",
      "Full eval scores so far: [20.75, 21.68]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 19 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.9019322857176402 / 5  (18.0): 100%|██████| 5/5 [00:16<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 18.04 on minibatch of size 5 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 2'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04]\n",
      "Full eval scores so far: [20.75, 21.68]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 20 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.8792427930270362 / 5  (17.6): 100%|██████| 5/5 [00:12<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 17.58 on minibatch of size 5 with parameters ['Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 3'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04, 17.58]\n",
      "Full eval scores so far: [20.75, 21.68]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "===== Full Eval 2 =====\n",
      "Doing full eval on next top averaging program (Avg Score: 26.63) from minibatch trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.171095713200966 / 20  (20.9): 100%|████| 20/20 [00:40<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full eval scores so far: [20.75, 21.68, 20.86]\n",
      "Best full score so far: 21.68\n",
      "=======================\n",
      "\n",
      "\n",
      "== Minibatch Trial 21 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.9461898125904971 / 5  (18.9): 100%|██████| 5/5 [00:16<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 18.92 on minibatch of size 5 with parameters ['Predictor 1: Instruction 3', 'Predictor 1: Few-Shot Set 4'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04, 17.58, 18.92]\n",
      "Full eval scores so far: [20.75, 21.68, 20.86]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 22 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.033247283900634 / 5  (20.7): 100%|███████| 5/5 [00:14<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 20.66 on minibatch of size 5 with parameters ['Predictor 1: Instruction 2', 'Predictor 1: Few-Shot Set 1'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04, 17.58, 18.92, 20.66]\n",
      "Full eval scores so far: [20.75, 21.68, 20.86]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 23 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.1217776067604932 / 5  (22.4): 100%|██████| 5/5 [00:19<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 22.44 on minibatch of size 5 with parameters ['Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 8'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04, 17.58, 18.92, 20.66, 22.44]\n",
      "Full eval scores so far: [20.75, 21.68, 20.86]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 24 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.325583187498081 / 5  (26.5): 100%|███████| 5/5 [00:10<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 26.51 on minibatch of size 5 with parameters ['Predictor 1: Instruction 4', 'Predictor 1: Few-Shot Set 1'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04, 17.58, 18.92, 20.66, 22.44, 26.51]\n",
      "Full eval scores so far: [20.75, 21.68, 20.86]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 25 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.8971821927711912 / 5  (17.9): 100%|██████| 5/5 [00:09<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 17.94 on minibatch of size 5 with parameters ['Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 1'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04, 17.58, 18.92, 20.66, 22.44, 26.51, 17.94]\n",
      "Full eval scores so far: [20.75, 21.68, 20.86]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 26 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.8583115775170115 / 5  (17.2): 100%|██████| 5/5 [00:11<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 17.17 on minibatch of size 5 with parameters ['Predictor 1: Instruction 7', 'Predictor 1: Few-Shot Set 1'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04, 17.58, 18.92, 20.66, 22.44, 26.51, 17.94, 17.17]\n",
      "Full eval scores so far: [20.75, 21.68, 20.86]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 27 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.18448702135853 / 5  (23.7): 100%|████████| 5/5 [00:50<00:00, 10.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 23.69 on minibatch of size 5 with parameters ['Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 5'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04, 17.58, 18.92, 20.66, 22.44, 26.51, 17.94, 17.17, 23.69]\n",
      "Full eval scores so far: [20.75, 21.68, 20.86]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 28 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.1725157937100747 / 5  (23.5): 100%|██████| 5/5 [00:20<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 23.45 on minibatch of size 5 with parameters ['Predictor 1: Instruction 1', 'Predictor 1: Few-Shot Set 6'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04, 17.58, 18.92, 20.66, 22.44, 26.51, 17.94, 17.17, 23.69, 23.45]\n",
      "Full eval scores so far: [20.75, 21.68, 20.86]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 29 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.7327754782566983 / 5  (14.7): 100%|██████| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 14.66 on minibatch of size 5 with parameters ['Predictor 1: Instruction 6', 'Predictor 1: Few-Shot Set 1'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04, 17.58, 18.92, 20.66, 22.44, 26.51, 17.94, 17.17, 23.69, 23.45, 14.66]\n",
      "Full eval scores so far: [20.75, 21.68, 20.86]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "== Minibatch Trial 30 / 30 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.0139890543500827 / 5  (20.3): 100%|██████| 5/5 [00:16<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 20.28 on minibatch of size 5 with parameters ['Predictor 1: Instruction 0', 'Predictor 1: Few-Shot Set 4'].\n",
      "Minibatch scores so far: [28.03, 21.24, 24.41, 18.55, 16.24, 23.54, 20.74, 28.61, 25.42, 18.95, 21.57, 19.21, 26.63, 16.02, 23.43, 22.29, 22.89, 20.04, 18.04, 17.58, 18.92, 20.66, 22.44, 26.51, 17.94, 17.17, 23.69, 23.45, 14.66, 20.28]\n",
      "Full eval scores so far: [20.75, 21.68, 20.86]\n",
      "Best full score so far: 21.68\n",
      "=============================\n",
      "\n",
      "\n",
      "===== Full Eval 3 =====\n",
      "Doing full eval on next top averaging program (Avg Score: 26.51) from minibatch trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.319258893787406 / 20  (21.6): 100%|████| 20/20 [00:40<00:00,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full eval scores so far: [20.75, 21.68, 20.86, 21.6]\n",
      "Best full score so far: 21.68\n",
      "=======================\n",
      "\n",
      "\n",
      "Returning best identified program with score 21.68!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer\n",
    "teleprompter = MIPROv2(\n",
    "    metric=r1,\n",
    "    #auto=\"light\", # Can choose between light, medium, and heavy optimization runs\n",
    ")\n",
    "\n",
    "# Optimize program\n",
    "print(f\"Optimizing program with MIPRO...\")\n",
    "optimized_program = teleprompter.compile(\n",
    "    shortsumm.deepcopy(),\n",
    "    trainset=trainset,\n",
    "    valset=devset,\n",
    "    minibatch_size=5,\n",
    "    max_bootstrapped_demos=3,\n",
    "    max_labeled_demos=4,\n",
    "    requires_permission_to_run=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimize program for future use\n",
    "optimized_program.save(f\"mipro_optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
